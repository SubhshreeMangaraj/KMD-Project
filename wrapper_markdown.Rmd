---
title: "Wrapper_markdown"
author: "Subhashree Mangaraj"
date: "25/10/2019"
output: html_document
---





```{r cars}
#----------------------Description-------------------------

#----------------------------------------------------------

#---------------------Loading of packages------------------
library(ParamHelpers)
library(mlbench)
library(mlr)
library(FSelector)
library(party)
library(kknn)
library(randomForest)
library(tgp)
library(stats)
library(FNN)
library(MASS)
library(readr)
library(here)
library(randomForestSRC)
library("parallelMap")
library(rlang)
#---------------------------------------------------------
```

```{r}
#--------------------Loading of data----------------------

df_data_reg <- readr::read_rds(file.path(here::here(), "adsl_adsl_sum-dur-regr_df.rds"))
df_data_clasf<- readr::read_rds(file.path(here::here(), "adsl_adsl_sum-dur-classif_df.rds"))


df_data_reg <- df_data_reg[, -c(1:7)]
df_data_reg$response_admission<-NULL

df_data_clasf <- df_data_clasf[, -c(1:7)]
df_data_clasf$response_admission<-NULL
df_data_clasf$response_org<-NULL
#---------------------------------------------------------

#------------Data Sampling--------------------------------

#Regression data
#df_samp_reg<-df_data_reg[sample(nrow(df_data_reg),100),]
#temp_tar<-df_samp_reg$response===
#df_samp_reg<-df_samp_reg[,sample(ncol(df_samp_reg),10)]
#df_samp_reg$response<-temp_tar

#classification data
#df_samp_cls<-df_data_clasf[sample(nrow(df_data_clasf),100),]
#temp_tarc<-df_samp_cls$response
#df_samp_cls<-df_samp_cls[,sample(ncol(df_samp_cls),10)]
#df_samp_cls$response<-temp_tarc

df_samp_reg <- data.frame(df_data_reg)
df_samp_cls <- data.frame(df_data_clasf)

#--------------------------------------------------------
```

```{r}
#-----------Model Training and Testing-------------------
modeltest<- function(df_tune, typeM){
  
  if(typeM == 'R'){#Regression
    
    regr.task_mod<-makeRegrTask( data = data.frame(df_tune), target = "response")
    n1 = getTaskSize(regr.task_mod)
    
    # Splitting the observations for training
    train.mod = seq(1, n1, by = 2)
    test.mod = seq(2, n1, by = 2)
    
    # Train the learner
    mod = mlr::train("regr.lm", regr.task_mod, subset = train.mod)
    mod
    
    task.pred = predict(mod, task = regr.task_mod, subset = test.mod)
    task.pred
    res<-performance(task.pred, measures = mse)
    
    
    return(round(res,digits = 3))
    
  }
  
  if(typeM == 'C'){#Classification
    
    clsf.task_mod<-makeClassifTask( data = data.frame(df_tune), target = "response")
    n2 = getTaskSize(clsf.task_mod)
    
    # Splitting the observations for training
    train.mod = seq(1, n2, by = 2)
    test.mod = seq(2, n2, by = 2)
    
    # Train the learner
    mod = mlr::train("classif.cforest", clsf.task_mod, subset = train.mod)
    mod
    task.pred = predict(mod, task = clsf.task_mod, subset = test.mod)
    task.pred
    res<-performance(task.pred, measures = acc)
    
    return(round(res,digits = 3))
    
  }
}
#--------------------------------------------------------
```

```{r}
#----------Common data: learner--------------------------
df_samp_reg<-df_samp_reg[(1:50),(1:10)]
df_samp_cls<-df_samp_cls[(1:50),(1:10)]
#Regression:Task Creation
regr.task<-makeRegrTask(id     = "Tin_regr", 
                        data   = data.frame(df_samp_reg), 
                        target = "response")
getTaskFeatureNames(regr.task)

#Repeated cross validation
cv.folds <- makeResampleDesc("CV", iters = 3)

# Define model tuning algorithm ~ Random tune algorithm
random.tune <- makeTuneControlRandom(maxit = 5)

#Classification:Task Creation
clsf.task<-makeClassifTask(id     = "Tin_clsf", 
                           data   = data.frame(df_samp_cls), 
                           target = "response")
getTaskFeatureNames(clsf.task)

ctrl = makeTuneControlGrid()
inner = makeResampleDesc("Holdout")
outer = makeResampleDesc("CV", iters = 3)
#--------------------------------------------------------
```

```{r}
#----------W:1.makeFeatSelControlRandom-----------------

control1<-makeFeatSelControlRandom(same.resampling.instance = TRUE,
                                   maxit                = 20L)

```

```{r}

##REGRESSION
#-----------

#---------1.1 R:K-Nearest-Neighbor regressiong-----------

#Tuning hyperparameters
lrnr<-makeLearner("regr.kknn")
#Get parameter set of the learner
getParamSet(lrnr)

#Getting tuned results 
ps <- makeParamSet(makeIntegerParam("k",       lower = 1, upper = 10),
                   makeNumericParam("distance",lower = 1, upper = 10))

lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)

modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the tuned parameters to make the learner
lrnr<-makeLearner("regr.kknn",
                  par.vals = list(k=10, distance=2))

#Defining learner model in the wrapper method
lrn_wr1<- makeFeatSelWrapper(learner    = lrnr, 
                             resampling = cv.folds,
                             control    = control1, 
                             show.info  = FALSE)

#Training of model to extract features
mod_wr1 = mlr::train(lrn_wr1, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_wr1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w1r1<-df_samp_reg
df_tune_w1r1<-df_tune_w1r1[, names(df_tune_w1r1) %in% cols] 
ncol(df_tune_w1r1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w1r1<-modeltest(df_tune_w1r1, 'R')
print(res_w1r1)

#Capturing best values of hyperparameter after tuning
df_val_w1r1<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w1r1)-1))
df_val_w1r1 = rbind(df_val_w1r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = 10)
df_val_w1r1 = rbind(df_val_w1r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "distance", Value = 2)
df_val_w1r1 = rbind(df_val_w1r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = res_w1r1)
df_val_w1r1 = rbind(df_val_w1r1,nwln, stringsAsFactors=FALSE)

print(df_val_w1r1)

write.csv(df_val_w1r1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1r1.csv")
write.csv(df_tune_w1r1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1r1_data.csv")

#---------End of Nearest-Neighbor regressiong------------
```

```{r}

#---------1.2 R:Conditional Inference Trees--------------

#Tuning hyperparameters
lrnr<-makeLearner("regr.ctree")
#Get parameter set of the learner
getParamSet(lrnr)

ps <- makeParamSet(makeDiscreteParam("teststat",    values = c("quad")),
                   makeNumericParam("mincriterion", lower = 0, upper = 1),
                   #makeIntegerParam("minsplit",     lower = 1, upper = 50), #remove cmt for org data
                   makeIntegerParam("minbucket",    lower = 4, upper = 8),
                   makeIntegerParam("maxsurrogate", lower = 0, upper = 5),
                   makeIntegerParam("mtry",         lower = 0, upper = 5)
                   #makeIntegerParam("maxdepth",     lower = 0, upper = 10)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)

modw = train(lrnr, regr.task)

print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.ctree",
                  par.vals = list(teststat= "quad", 
                                  mincriterion=0.778,
                                  minbucket=5,
                                  maxsurrogate=1,
                                  mtry=3))

#Defining learner model
lrn_w1r2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w1r2 = mlr::train(lrn_w1r2, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w1r2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w1r2<-df_samp_reg
df_tune_w1r2<-df_tune_w1r2[, names(df_tune_w1r2) %in% cols] 
ncol(df_tune_w1r2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w1r2<-modeltest(df_tune_w1r2, 'R')
print(res_w1r2)

#Capturing best values of hyperparameter after tuning
df_val_w1r2<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w1r2)-1))
df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "teststat", Value = tuned.model_r2$x$teststat)
df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mincriterion", Value = 0.778)
df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "minsplit", Value = tuned.model_r2$x$minsplit)
#df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "minbucket", Value = 5)
df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "maxsurrogate", Value = 1)
df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mtry", Value = 3)
df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "maxdepth", Value = tuned.model_r2$x$maxdepth)
#df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = res_w1r2)
df_val_w1r2 = rbind(df_val_w1r2,nwln, stringsAsFactors=FALSE)

print(df_val_w1r2)

write.csv(df_val_w1r2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1r2.csv")
write.csv(df_tune_w1r2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1r2_data.csv")

#---------End of Conditional Inference Trees------------
```

```{r}

#---------1.3 R:Bayesian CART---------------------------

#Tuning hyperparameters
lrnr<-makeLearner("regr.bcart")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("bprior",values = c("b0","b0not","bflat","bmle","bmznot","bmzt")),
                   makeIntegerParam("R",      lower = 1, upper = 10),
                   makeIntegerParam("verb",   lower = 0, upper = 4)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.bcart",
                  par.vals = list(bprior= "bflat", 
                                  R=6,
                                  verb=4))


#Defining learner model
lrn_w1r3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w1r3 = mlr::train(lrn_w1r3, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w1r3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w1r3<-df_samp_reg
df_tune_w1r3<-df_tune_w1r3[, names(df_tune_w1r3) %in% cols] 
ncol(df_tune_w1r3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w1r3<-modeltest(df_tune_w1r3, 'R')
print(res_w1r3)

#Capturing best values of hyperparameter after tuning
df_val_w1r3<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w1r3)-1))
df_val_w1r3 = rbind(df_val_w1r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "bprior", Value = "bflat")
df_val_w1r3 = rbind(df_val_w1r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "R", Value = 6)
df_val_w1r3 = rbind(df_val_w1r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "verb", Value = 4)
df_val_w1r3 = rbind(df_val_w1r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = round(res_w1r3,3))
df_val_w1r3 = rbind(df_val_w1r3,nwln, stringsAsFactors=FALSE)


print(df_val_w1r3)

write.csv(df_val_w1r3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1r3.csv")
write.csv(df_tune_w1r3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1r3_data.csv")

#---------End of Bayesian CART---------------------------
```

```{r}
################start phase 2 run##################3
##CLASSIFICATION
#---------------

#---------2.1 C:Binomial Regression----------------------
#Tuning hyperparameters
lrnr<-makeLearner("classif.binomial")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("link",values = c("logit","probit","cloglog","cauchit"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = mlr::train(lrnr, clsf.task)
print(getTuneResult(modw))
r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.binomial",
                  par.vals = list(link="probit"))


#Defining learner model
lrn_w1c1<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w1c1 = mlr::train(lrn_w1c1, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w1c1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w1c1<-df_samp_cls
df_tune_w1c1<-df_tune_w1c1[, names(df_tune_w1c1) %in% cols] 
ncol(df_tune_w1c1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w1c1<-modeltest(df_tune_w1c1, 'C')
print(res_w1c1)

#Capturing best values of hyperparameter after tuning
df_val_w1c1<-data.frame(Observation=character(), Value=character(), Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w1c1)-1), Range = (ncol(df_data_clasf)-1))
df_val_w1c1 = rbind(df_val_w1c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "link", Value = tuned.model_c1$x$link, Range = "logit, probit ,cloglog ,cauchit")
df_val_w1c1 = rbind(df_val_w1c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = res_w1c1, Range="")
df_val_w1c1 = rbind(df_val_w1c1,nwln, stringsAsFactors=FALSE)

print(df_val_w1c1)

write.csv(df_val_w1c1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1c1.csv")
write.csv(df_tune_w1c1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1c1_data.csv")

#---------End of Binomial Regression---------------------
```

```{r}

#---------2.2 C:Fast k-Nearest Neighbour-----------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.fnn")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeIntegerParam( "k",     lower = 1, upper = 10),
                   makeDiscreteParam("algorithm",values = c("cover_tree","kd_tree","brute"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.fnn",
                  par.vals = list(k=9,
                                  algorithm="brute"))

#Defining learner model
lrn_w1c2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w1c2 = mlr::train(lrn_w1c2, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w1c2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w1c2<-df_samp_cls
df_tune_w1c2<-df_tune_w1c2[, names(df_tune_w1c2) %in% cols] 
ncol(df_tune_w1c2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w1c2<-modeltest(df_tune_w1c2, 'C')
print(res_w1c2)

#Capturing best values of hyperparameter after tuning
df_val_w1c2<-data.frame(Observation=character(), Value=character(), Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w1c2)-1), Range=(ncol(df_samp_cls)-1))
df_val_w1c2 = rbind(df_val_w1c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = 9, Range = "1-10")
df_val_w1c2 = rbind(df_val_w1c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "algorithm", Value = "brute", Range ="cover_tree, kd_tree, brute")
df_val_w1c2 = rbind(df_val_w1c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = round(res_w1c2,3),Range="")
df_val_w1c2 = rbind(df_val_w1c2,nwln, stringsAsFactors=FALSE)


print(df_val_w1c2)

write.csv(df_val_w1c2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1c2.csv")
write.csv(df_tune_w1c2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1c2_data.csv")

#---------End of Fast k-Nearest Neighbour----------------
```

```{r}

#---------2.3 C:Linear Discriminant Analysis-------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.lda")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeNumericParam( "nu",     lower = 2, upper = 10),
                   #makeNumericParam( "tol",    lower = 0, upper = 0.0005),
                   makeDiscreteParam("predict.method",values = c("plug-in","predictive","debiased"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.lda",
                  par.vals = list(nu=4.67,
                                 # tot=0.0005,
                                  predict.method="predictive"))

#Defining learner model
lrn_w1c3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w1c3 = mlr::train(lrn_w1c3, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w1c3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w1c3<-df_samp_cls
df_tune_w1c3<-df_tune_w1c3[, names(df_tune_w1c3) %in% cols] 
ncol(df_tune_w1c3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w1c3<-modeltest(df_tune_w1c3, 'C')
print(res_w1c3)

#Capturing best values of hyperparameter after tuning
df_val_w1c3<-data.frame(Hyperparameter=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w1c3)-1),Range=(ncol(df_samp_cls)-1))
df_val_w1c3 = rbind(df_val_w1c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "nu", Value = 4.67,Range="2-10")
df_val_w1c3 = rbind(df_val_w1c3,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "tol", Value = tuned.model_c3$x$tol)
#df_val_w1c3 = rbind(df_val_w1c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "predict.method", Value = "predictive",Range="plug-in,predictive,debiased")
df_val_w1c3 = rbind(df_val_w1c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = res_w1c3,Range="")
df_val_w1c3 = rbind(df_val_w1c3,nwln, stringsAsFactors=FALSE)

print(df_val_w1c3)

write.csv(df_val_w1c3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1c3.csv")
write.csv(df_tune_w1c3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w1c3_data.csv")


#---------End of Linear Discriminant Analysis------------
#--------------------------------------------------------
```

```{r}

#----------W:2.makeFeatSelControlGA----------------------

control1<-makeFeatSelControlGA(same.resampling.instance = TRUE,
                               maxit                = 20L)
```

```{r}

##REGRESSION
#-----------

#---------2.1 R:K-Nearest-Neighbor regressiong-----------
#Tuning hyperparameters
lrnr<-makeLearner("regr.kknn")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeIntegerParam("k",       lower = 1, upper = 10),
                   makeNumericParam("distance",lower = 1, upper = 10)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.kknn",
                  par.vals = list(k=10, distance=1))


#Defining learner model
lrn_w2r1<- makeFeatSelWrapper(learner   = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w2r1 = mlr::train(lrn_w2r1, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w2r1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w2r1<-df_samp_reg
df_tune_w2r1<-df_tune_w2r1[, names(df_tune_w2r1) %in% cols] 
ncol(df_tune_w2r1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w2r1<-modeltest(df_tune_w2r1, 'R')
print(res_w2r1)

#Capturing best values of hyperparameter after tuning
df_val_w2r1<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2r1)-1),Range=(ncol(df_samp_reg)-1))
df_val_w2r1 = rbind(df_val_w2r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = 10,Range="1-10")
df_val_w2r1 = rbind(df_val_w2r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "distance", Value = 1, Range="1-10")
df_val_w2r1 = rbind(df_val_w2r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = round(res_w2r1,3),Range="")
df_val_w2r1 = rbind(df_val_w2r1,nwln, stringsAsFactors=FALSE)

print(df_val_w2r1)

write.csv(df_val_w2r1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2r1.csv")
write.csv(df_tune_w2r1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2r1_data.csv")

#---------End of Nearest-Neighbor regressiong------------
```

```{r}

#---------2.2 R:Conditional Inference Trees--------------
#Tuning hyperparameters
lrnr<-makeLearner("regr.ctree")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("teststat",    values = c("quad","max")),
                   #makeNumericParam("mincriterion", lower = 0, upper = 1),
                   makeIntegerParam("minsplit",     lower = 1, upper = 50), #remove cmt for org data
                   #makeIntegerParam("minbucket",    lower = 1, upper = 10),
                   makeIntegerParam("maxsurrogate", lower = 0, upper = 10),
                   makeIntegerParam("mtry",         lower = 0, upper = 5)
                   #makeIntegerParam("maxdepth",     lower = 0, upper = 10)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.ctree",
                  par.vals = list(teststat="max",
                                  minsplit=1,
                                  maxsurrogate=4,
                                  mtry=0))

#Defining learner model
lrn_w2r2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w2r2 = mlr::train(lrn_w2r2, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w2r2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w2r2<-df_samp_reg
df_tune_w2r2<-df_tune_w2r2[, names(df_tune_w2r2) %in% cols] 
ncol(df_tune_w2r2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w2r2<-modeltest(df_tune_w2r2, 'R')
print(res_w2r2)

#Capturing best values of hyperparameter after tuning
df_val_w2r2<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2r2)-1),Range=(ncol(df_samp_reg)-1))
df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "teststat", Value = "max", Range= "quad,max")
df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "mincriterion", Value = tuned.model_r2$x$mincriterion)
#df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "minsplit", Value = as.integer(1), Range="1-50")
df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "minbucket", Value = tuned.model_r2$x$minbucket)
#df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "maxsurrogate", Value = as.integer(4),Range="0-10")
df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mtry", Value = as.integer(0),Range="0-5")
df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "maxdepth", Value = tuned.model_r2$x$maxdepth)
#df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = round(res_w2r2,3),Range="")
df_val_w2r2 = rbind(df_val_w2r2,nwln, stringsAsFactors=FALSE)

print(df_val_w2r2)

write.csv(df_val_w2r2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2r2.csv")
write.csv(df_tune_w2r2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2r2_data.csv")

#---------End of Conditional Inference Trees------------
```

```{r}

#---------2.3 R:Bayesian CART---------------------------
#Tuning hyperparameters
lrnr<-makeLearner("regr.bcart")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("bprior",values = c("b0","b0not","bflat","bmle","bmznot","bmzt")),
                   makeIntegerParam("R",      lower = 1, upper = 10),
                   makeIntegerParam("verb",   lower = 0, upper = 4)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.bcart",
                  par.vals = list(bprior="bflat", 
                                  R=8,
                                  verb=1))

#Defining learner model
lrn_w2r3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w2r3 = mlr::train(lrn_w2r3, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w2r3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w2r3<-df_samp_reg
df_tune_w2r3<-df_tune_w2r3[, names(df_tune_w2r3) %in% cols] 
ncol(df_tune_w2r3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w2r3<-modeltest(df_tune_w2r3, 'R')
print(res_w2r3)

#Capturing best values of hyperparameter after tuning
df_val_w2r3<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2r3)-1), Range=(ncol(df_samp_reg)-1))
df_val_w2r3 = rbind(df_val_w2r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "bprior", Value = bflat, Range="b0,b0not,bflat,bmle,bmznot,bmzt")
df_val_w2r3 = rbind(df_val_w2r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "R", Value = "8", Range = "1-10")
df_val_w2r3 = rbind(df_val_w2r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "verb", Value = "1", Range= "0-4")
df_val_w2r3 = rbind(df_val_w2r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = round(res_w2r3,3), Range= "")
df_val_w2r3 = rbind(df_val_w2r3,nwln, stringsAsFactors=FALSE)

print(df_val_w2r3)


write.csv(df_val_w2r3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2r3.csv")
write.csv(df_tune_w2r3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2r3_data.csv")


#---------End of Bayesian CART---------------------------
```

```{r}

##CLASSIFICATION
#---------------

#---------2.1 C:Binomial Regression----------------------
#Tuning hyperparameters
lrnr<-makeLearner("classif.binomial")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("link",values = c("logit","probit","cloglog","cauchit"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.binomial",
                   par.vals = list(link="probit"))




#Defining learner model
lrn_w2c1<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info = FALSE)

#Training of model to extract features
mod_w2c1 = mlr::train(lrn_w2c1, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w2c1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w2c1<-df_samp_cls
df_tune_w2c1<-df_tune_w2c1[, names(df_tune_w2c1) %in% cols] 
ncol(df_tune_w2c1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w2c1<-modeltest(df_tune_w2c1, 'C')
print(res_w2c1)

#Capturing best values of hyperparameter after tuning
df_val_w2c1<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2c1)-1),Range=(ncol(df_samp_cls)-1))
df_val_w2c1 = rbind(df_val_w2c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "link", Value = "probit",Range="logit,probit,cloglog,cauchit" )
df_val_w2c1 = rbind(df_val_w2c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = round(res_w2c1,3),Range="")
df_val_w2c1 = rbind(df_val_w2c1,nwln, stringsAsFactors=FALSE)

print(df_val_w2c1)

write.csv(df_val_w2c1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2c1.csv")
write.csv(df_tune_w2c1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2c1_data.csv")

#---------End of Binomial Regression---------------------
```

```{r}

#---------2.2 C:Fast k-Nearest Neighbour-----------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.fnn")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeIntegerParam( "k",     lower = 1, upper = 10),
                   makeDiscreteParam("algorithm",values = c("cover_tree","kd_tree","brute"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the tuned parameters to make the learner
lrnr<-makeLearner("classif.fnn",
                  par.vals = list(k=9,
                                  algorithm="brute"))

#Defining learner model
lrn_w2c2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w2c2 = mlr::train(lrn_w2c2, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w2c2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w2c2<-df_samp_cls
df_tune_w2c2<-df_tune_w2c2[, names(df_tune_w2c2) %in% cols] 
ncol(df_tune_w2c2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w2c2<-modeltest(df_tune_w2c2, 'C')
print(res_w2c2)

#Capturing best values of hyperparameter after tuning
df_val_w2c2<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2c2)-1),Range=(ncol(df_samp_cls)-1))
df_val_w2c2 = rbind(df_val_w2c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = "9", Range="1-10")
df_val_w2c2 = rbind(df_val_w2c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "algorithm", Value = "brute",Range="cover_tree, kd_tree, brute")
df_val_w2c2 = rbind(df_val_w2c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = round(res_w2c2,3),Range="")
df_val_w2c2 = rbind(df_val_w2c2,nwln, stringsAsFactors=FALSE)


print(df_val_w2c2)
write.csv(df_val_w2c2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2c2.csv")
write.csv(df_tune_w2c2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2c2_data.csv")

#---------End of Fast k-Nearest Neighbour----------------
```

```{r}

#---------2.3 C:Linear Discriminant Analysis-------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.lda")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeNumericParam( "nu",     lower = 2, upper = 10),
                   makeNumericParam( "tol",    lower = 0, upper = 0.0005),
                   makeDiscreteParam("predict.method",values = c("plug-in","predictive","debiased"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.lda",
                  par.vals = list(nu=4.67,
                                  # tot=0.0005,
                                  predict.method="predictive"))

#Defining learner model
lrn_w2c3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w2c3 = mlr::train(lrn_w2c3, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w2c3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w2c3<-df_samp_cls
df_tune_w2c3<-df_tune_w2c3[, names(df_tune_w2c3) %in% cols] 
ncol(df_tune_w2c3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w2c3<-modeltest(df_tune_w2c3, 'C')
print(res_w2c3)

#Capturing best values of hyperparameter after tuning
df_val_w2c3<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2c3)-1),Range=(ncol(df_samp_cls)-1))
df_val_w2c3 = rbind(df_val_w2c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "nu", Value = "4.670",Range="2-10")
df_val_w2c3 = rbind(df_val_w2c3,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "tol", Value = tuned.model_c3$x$tol, )
#df_val_w2c3 = rbind(df_val_w2c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "predict.method", Value = "predictive", Range="plug-in,predictive,debiased")
df_val_w2c3 = rbind(df_val_w2c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = round(res_w2c3,3),Range="")
df_val_w2c3 = rbind(df_val_w2c3,nwln, stringsAsFactors=FALSE)

print(df_val_w2c3)
write.csv(df_val_w2c3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2c3.csv")
write.csv(df_tune_w2c3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w2c3_data.csv")
#---------End of Linear Discriminant Analysis------------
#--------------------------------------------------------
```

```{r}

#----------W:3.makeFeatSelControlSequential--------------

control1<-makeFeatSelControlSequential(same.resampling.instance = TRUE,
                                       impute.val = NULL, method = "sfs", alpha = 0.01, beta = -0.001,
                                       maxit = NA_integer_, max.features = NA_integer_,
                                       tune.threshold = FALSE, tune.threshold.args = list(),
                                       log.fun = "default")
```

```{r}

##REGRESSION
#-----------
#---------3.1 R:K-Nearest-Neighbor regressiong-----------
#Tuning hyperparameters
lrnr<-makeLearner("regr.kknn")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeIntegerParam("k",       lower = 1, upper = 10),
                   makeNumericParam("distance",lower = 1, upper = 10)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
#r = resample(lrnr, regr.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.kknn",
                  par.vals = list(k=10, distance=2))


#Defining learner model
lrn_w3r1<- makeFeatSelWrapper(learner   = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w3r1 = mlr::train(lrn_w3r1, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w3r1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w3r1<-df_samp_reg
df_tune_w3r1<-df_tune_w3r1[, names(df_tune_w3r1) %in% cols] 
ncol(df_tune_w3r1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w3r1<-modeltest(df_tune_w3r1, 'R')
print(res_w3r1)

#Capturing best values of hyperparameter after tuning
df_val_w3r1<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w3r1)-1),Range=(ncol(df_samp_reg)-1))
df_val_w3r1 = rbind(df_val_w3r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = "10",Range="1-10")
df_val_w3r1 = rbind(df_val_w3r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "distance", Value = "2",Range="1-10")
df_val_w3r1 = rbind(df_val_w3r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = round(res_w3r1,3),Range="")
df_val_w3r1 = rbind(df_val_w3r1,nwln, stringsAsFactors=FALSE)

print(df_val_w3r1)

write.csv(df_val_w3r1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3r1.csv")
write.csv(df_tune_w3r1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3r1_data.csv")

#---------End of Nearest-Neighbor regressiong------------
```

```{r}

#---------3.2 R:Conditional Inference Trees--------------
#Tuning hyperparameters
lrnr<-makeLearner("regr.ctree")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("teststat",    values = c("quad","max")),
                   makeNumericParam("mincriterion", lower = 0, upper = 1),
                   #makeIntegerParam("minsplit",     lower = 1, upper = 50), #remove cmt for org data
                   makeIntegerParam("minbucket",    lower = 1, upper = 10),
                   #makeIntegerParam("maxsurrogate", lower = 0, upper = 10),
                   makeIntegerParam("mtry",         lower = 0, upper = 5),
                   #makeIntegerParam("maxdepth",     lower = 0, upper = 10)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.ctree",
                  par.vals = list(teststat= "quad", 
                                  mincriterion=0.778,
                                  minbucket=5,
                                  mtry=3))

#Defining learner model
lrn_w3r2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w3r2 = mlr::train(lrn_w3r2, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w3r2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w3r2<-df_samp_reg
df_tune_w3r2<-df_tune_w3r2[, names(df_tune_w3r2) %in% cols] 
ncol(df_tune_w3r2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w3r2<-modeltest(df_tune_w3r2, 'R')
print(res_w3r2)

#Capturing best values of hyperparameter after tuning
df_val_w3r2<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w3r2)-1),Range=(ncol(df_samp_reg)-1))
df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "teststat", Value = "quad",Range="quad,max")
df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mincriterion", Value = "0.778",Range="0-1")
df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "minsplit", Value = tuned.model_r2$x$minsplit)
#df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "minbucket", Value = "5",Range="1-10")
df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "maxsurrogate", Value = tuned.model_r2$x$maxsurrogate)
#df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mtry", Value = "3",Range="0-5")
df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "maxdepth", Value = tuned.model_r2$x$maxdepth)
#df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = round(res_w3r2,3),Range="")
df_val_w3r2 = rbind(df_val_w3r2,nwln, stringsAsFactors=FALSE)

print(df_val_w3r2)

write.csv(df_val_w3r2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3r2.csv")
write.csv(df_tune_w3r2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3r2_data.csv")

#---------End of Conditional Inference Trees------------
```

```{r}

#---------3.3 R:Bayesian CART---------------------------
#Tuning hyperparameters
lrnr<-makeLearner("regr.bcart")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("bprior",values = c("b0","b0not","bflat","bmle","bmznot","bmzt")),
                   makeIntegerParam("R",      lower = 1, upper = 10),
                   makeIntegerParam("verb",   lower = 0, upper = 4)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.bcart",
                  par.vals = list(bprior= "bflat", 
                                  R=6,
                                  verb=4))

#Defining learner model
lrn_w3r3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w3r3 = mlr::train(lrn_w3r3, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w3r3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w3r3<-df_samp_reg
df_tune_w3r3<-df_tune_w3r3[, names(df_tune_w3r3) %in% cols] 
ncol(df_tune_w3r3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w3r3<-modeltest(df_tune_w3r3, 'R')
print(res_w3r3)

#Capturing best values of hyperparameter after tuning
df_val_w3r3<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w3r3)-1),Range=(ncol(df_samp_reg)-1))
df_val_w3r3 = rbind(df_val_w3r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "bprior", Value = "bflat",Range="b0,b0not,bflat,bmle,bmznot,bmzt")
df_val_w3r3 = rbind(df_val_w3r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "R", Value = "6",Range="1-10")
df_val_w3r3 = rbind(df_val_w3r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "verb", Value = "4",Range="0-4")
df_val_w3r3 = rbind(df_val_w3r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = round(res_w3r3,3),Range="")
df_val_w3r3 = rbind(df_val_w3r3,nwln, stringsAsFactors=FALSE)

print(df_val_w3r3)

write.csv(df_val_w3r3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3r3.csv")
write.csv(df_tune_w3r3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3r3_data.csv")


#---------End of Bayesian CART---------------------------
```

```{r}

##CLASSIFICATION
#---------------

#---------3.1 C:Binomial Regression----------------------
#Tuning hyperparameters
lrnr<-makeLearner("classif.binomial")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("link",values = c("logit","probit","cloglog","cauchit","log"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.binomial",
                  par.vals = list(link="probit"))


#Defining learner model
lrn_w3c1<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info = FALSE)

#Training of model to extract features
mod_w3c1 = mlr::train(lrn_w3c1, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w3c1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w3c1<-df_samp_cls
df_tune_w3c1<-df_tune_w3c1[, names(df_tune_w3c1) %in% cols] 
ncol(df_tune_w3c1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w3c1<-modeltest(df_tune_w3c1, 'C')
print(res_w3c1)

#Capturing best values of hyperparameter after tuning
df_val_w3c1<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2c1)-1),Range=(ncol(df_samp_cls)-1))
df_val_w3c1 = rbind(df_val_w3c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "link", Value = "probit",Range="logit,probit,cloglog,cauchit")
df_val_w3c1 = rbind(df_val_w3c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = round(res_w3c1,3),Range="")
df_val_w3c1 = rbind(df_val_w3c1,nwln, stringsAsFactors=FALSE)

print(df_val_w3c1)

write.csv(df_val_w3c1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3c1.csv")
write.csv(df_tune_w3c1, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3c1_data.csv")

#---------End of Binomial Regression---------------------
```

```{r}

#---------3.2 C:Fast k-Nearest Neighbour-----------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.fnn")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeIntegerParam( "k",     lower = 1, upper = 10),
                   makeDiscreteParam("algorithm",values = c("cover_tree","kd_tree","brute"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the tuned parameters to make the learner
lrnr<-makeLearner("classif.fnn",
                  par.vals = list(k=9,
                                  algorithm="brute"))

#Defining learner model
lrn_w3c2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w3c2 = mlr::train(lrn_w3c2, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w3c2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w3c2<-df_samp_cls
df_tune_w3c2<-df_tune_w3c2[, names(df_tune_w3c2) %in% cols] 
ncol(df_tune_w3c2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w3c2<-modeltest(df_tune_w3c2, 'C')
print(res_w3c2)

#Capturing best values of hyperparameter after tuning
df_val_w3c2<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w3c2)-1),Range=(ncol(df_samp_cls)-1))
df_val_w3c2 = rbind(df_val_w3c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = "9",Range="1-10")
df_val_w3c2 = rbind(df_val_w3c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "algorithm", Value = "brute",Range="cover_tree,kd_tree,brute")
df_val_w3c2 = rbind(df_val_w3c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = round(res_w3c2,3),Range="")
df_val_w3c2 = rbind(df_val_w3c2,nwln, stringsAsFactors=FALSE)


print(df_val_w3c2)

write.csv(df_val_w3c2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3c2.csv")
write.csv(df_tune_w3c2, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3c2_data.csv")


#---------End of Fast k-Nearest Neighbour----------------
```

```{r}

#---------3.3 C:Linear Discriminant Analysis-------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.lda")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeNumericParam( "nu",     lower = 2, upper = 10),
                   #makeNumericParam( "tol",    lower = 0, upper = 0.0005),
                   makeDiscreteParam("predict.method",values = c("plug-in","predictive","debiased"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.lda",
                  par.vals = list(nu=2,
                                  # tot=0.0005,
                                  predict.method="predictive"))

#Defining learner model
lrn_w3c3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w3c3 = mlr::train(lrn_w3c3, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w3c3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w3c3<-df_samp_cls
df_tune_w3c3<-df_tune_w3c3[, names(df_tune_w3c3) %in% cols] 
ncol(df_tune_w3c3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w3c3<-modeltest(df_tune_w3c3, 'C')
print(res_w3c3)

#Capturing best values of hyperparameter after tuning
df_val_w3c3<-data.frame(Observation=character(), Value=character(),Range=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w3c3)-1),Range=(ncol(df_samp_cls)-1))
df_val_w3c3 = rbind(df_val_w3c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "nu", Value = "2",Range="2-10")
df_val_w3c3 = rbind(df_val_w3c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "predict.method", Value = "predictive",Range="plug-in,predictive,debiased")
df_val_w3c3 = rbind(df_val_w3c3,nwln, stringsAsFactors=FALSE)


nwln<- list(Observation= "Accuracy", Value = round(res_w3c3,3),Range="")
df_val_w3c3 = rbind(df_val_w3c3,nwln, stringsAsFactors=FALSE)

print(df_val_w3c3)

write.csv(df_val_w3c3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3c3.csv")
write.csv(df_tune_w3c3, file = "/Users/MANAS MANGARAJ/Documents/wrapperoutput/df_val_w3c3_data.csv")

#---------End of Linear Discriminant Analysis------------
#--------------------------------------------------------
```

```{r}

#----------W:4.makeFeatSelControlExhaustive--------------

control1<-makeFeatSelControlExhaustive(same.resampling.instance = TRUE,
                                       maxit                = 20L)
```

```{r}
##REGRESSION
#-----------
#---------4.1 R:K-Nearest-Neighbor regressiong-----------

#Tuning hyperparameters
lrnr<-makeLearner("regr.kknn")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeIntegerParam("k",       lower = 1, upper = 10),
                   makeNumericParam("distance",lower = 1, upper = 10)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
#r = resample(lrnr, regr.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.kknn",
                  par.vals = list(k=10, distance=2))


#Defining learner model
lrn_w4r1<- makeFeatSelWrapper(learner   = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
parallelStartSocket(5)
mod_w4r1 = mlr::train(lrn_w4r1, task = regr.task)
parallelStop()
#Extract the features
sfeats = getFeatSelResult(mod_w4r1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w4r1<-df_samp_reg
df_tune_w4r1<-df_tune_w4r1[, names(df_tune_w4r1) %in% cols] 
ncol(df_tune_w4r1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w4r1<-modeltest(df_tune_w4r1, 'R')
print(res_w4r1)

#Capturing best values of hyperparameter after tuning
df_val_w4r1<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w4r1)-1))
df_val_w4r1 = rbind(df_val_w4r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = 10)
df_val_w4r1 = rbind(df_val_w4r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "distance", Value = 1)
df_val_w4r1 = rbind(df_val_w4r1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = res_w4r1)
df_val_w4r1 = rbind(df_val_w4r1,nwln, stringsAsFactors=FALSE)

print(df_val_w4r1)

#---------End of Nearest-Neighbor regressiong------------
```

```{r}
#---------3.2 R:Conditional Inference Trees--------------
#Tuning hyperparameters
#lrnr<-makeLearner("regr.ctree")
#Get parameter set of the learner
#getParamSet(lrnr)
#ps <- makeParamSet(makeDiscreteParam("teststat",    values = c("quad","max")),
#                   makeNumericParam("mincriterion", lower = 0, upper = 1),
 #                  makeIntegerParam("minsplit",     lower = 1, upper = 50), #remove cmt for org data
#                   makeIntegerParam("minbucket",    lower = 1, upper = 10),
#                   makeIntegerParam("maxsurrogate", lower = 0, upper = 10),
#                   makeIntegerParam("mtry",         lower = 0, upper = 5),
#                   makeIntegerParam("maxdepth",     lower = 0, upper = 10)
#)
#lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
#modw = train(lrnr, regr.task)
#print(getTuneResult(modw))
#r = resample(lrnr, regr.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.ctree",
                  par.vals = list(teststat= "quad"))
 #                                 mincriterion=0.778,
#                                  minbucket=5))
#                                  maxsurrogate=1))
#                                  mtry=3))

#Defining learner model
lrn_w4r2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w4r2 = mlr::train(lrn_w4r2, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w4r2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w4r2<-df_samp_reg
df_tune_w4r2<-df_tune_w4r2[, names(df_tune_w4r2) %in% cols] 
ncol(df_tune_w4r2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w4r2<-modeltest(df_tune_w4r2, 'R')
print(res_w4r2)

#Capturing best values of hyperparameter after tuning
df_val_w4r2<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w4r2)-1))
df_val_w4r2 = rbind(df_val_w4r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "teststat", Value = tuned.model_r2$x$teststat)
#df_val_w4r2 = rbind(df_val_w4r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "mincriterion", Value = tuned.model_r2$x$mincriterion)
#df_val_w4r2 = rbind(df_val_w4r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "minbucket", Value = tuned.model_r2$x$minbucket)
#df_val_w4r2 = rbind(df_val_w4r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "maxsurrogate", Value = tuned.model_r2$x$maxsurrogate)
#df_val_w4r2 = rbind(df_val_w4r2,nwln, stringsAsFactors=FALSE)

#nwln<- list(Observation= "mtry", Value = tuned.model_r2$x$mtry)
#df_val_w4r2 = rbind(df_val_w4r2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = res_w4r2)
df_val_w4r2 = rbind(df_val_w4r2,nwln, stringsAsFactors=FALSE)

Print(df_val_w4r2)
#---------End of Conditional Inference Trees------------
```

```{r}
#---------3.3 R:Bayesian CART---------------------------
#Tuning hyperparameters
lrnr<-makeLearner("regr.bcart")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("bprior",values = c("b0","b0not","bflat","bmle","bmznot","bmzt")),
                   makeIntegerParam("R",      lower = 1, upper = 10),
                   makeIntegerParam("verb",   lower = 0, upper = 4)
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, regr.task)
print(getTuneResult(modw))
r = resample(lrnr, regr.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("regr.bcart",
                  par.vals = list(bprior=10, 
                                  R=1,
                                  verb=2))

#Defining learner model
lrn_w4r3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w4r3 = mlr::train(lrn_w4r3, task = regr.task)

#Extract the features
sfeats = getFeatSelResult(mod_w4r3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w4r3<-df_samp_reg
df_tune_w4r3<-df_tune_w4r3[, names(df_tune_w4r3) %in% cols] 
ncol(df_tune_w4r3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w4r3<-modeltest(df_tune_w4r3, 'R')
print(res_w4r3)

#Capturing best values of hyperparameter after tuning
df_val_w4r3<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w4r3)-1))
df_val_w4r3 = rbind(df_val_w4r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "bprior", Value = tuned.model_r3$x$bprior)
df_val_w4r3 = rbind(df_val_w4r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "R", Value = tuned.model_r3$x$R)
df_val_w4r3 = rbind(df_val_w4r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "verb", Value = tuned.model_r3$x$verb)
df_val_w4r3 = rbind(df_val_w4r3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "mse", Value = res_w4r3)
df_val_w4r3 = rbind(df_val_w4r3,nwln, stringsAsFactors=FALSE)

print(df_val_w4r3)

#---------End of Bayesian CART---------------------------
```

```{r}
##CLASSIFICATION
#---------------

#---------3.1 C:Binomial Regression----------------------
#Tuning hyperparameters
lrnr<-makeLearner("classif.binomial")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeDiscreteParam("link",values = c("logit","probit","cloglog","cauchit"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.binomial",
                  par.vals = list(link="cloglog"))


#Defining learner model
lrn_w4c1<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info = FALSE)

#Training of model to extract features
mod_w4c1 = mlr::train(lrn_w4c1, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w4c1)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w4c1<-df_samp_cls
df_tune_w4c1<-df_tune_w4c1[, names(df_tune_w4c1) %in% cols] 
ncol(df_tune_w4c1)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w4c1<-modeltest(df_tune_w4c1, 'C')
print(res_w4c1)

#Capturing best values of hyperparameter after tuning
df_val_w4c1<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w2c1)-1))
df_val_w4c1 = rbind(df_val_w4c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "link", Value = tuned.model_c1$x$link)
df_val_w4c1 = rbind(df_val_w4c1,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = res_w4c1)
df_val_w4c1 = rbind(df_val_w4c1,nwln, stringsAsFactors=FALSE)

print(df_val_w4c1)
#---------End of Binomial Regression---------------------
```

```{r}
#---------3.2 C:Fast k-Nearest Neighbour-----------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.fnn")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeIntegerParam( "k",     lower = 1, upper = 10),
                   makeDiscreteParam("algorithm",values = c("cover_tree","kd_tree","brute"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the tuned parameters to make the learner
lrnr<-makeLearner("classif.fnn",
                  par.vals = list(k=10,
                                  algorithm="kd_tree"))

#Defining learner model
lrn_w4c2<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w4c2 = mlr::train(lrn_w4c2, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w4c2)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w4c2<-df_samp_cls
df_tune_w4c2<-df_tune_w4c2[, names(df_tune_w4c2) %in% cols] 
ncol(df_tune_w4c2)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w4c2<-modeltest(df_tune_w4c2, 'C')
print(res_w4c2)

#Capturing best values of hyperparameter after tuning
df_val_w4c2<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w4c2)-1))
df_val_w4c2 = rbind(df_val_w4c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "k", Value = tuned.model_c2$x$k)
df_val_w4c2 = rbind(df_val_w4c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "algorithm", Value = tuned.model_c2$x$algorithm)
df_val_w4c2 = rbind(df_val_w4c2,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = res_w4c2)
df_val_w4c2 = rbind(df_val_w4c2,nwln, stringsAsFactors=FALSE)


print(df_val_w4c2)

#---------End of Fast k-Nearest Neighbour----------------
```

```{r}
#---------3.3 C:Linear Discriminant Analysis-------------

#Tuning hyperparameters
lrnr<-makeLearner("classif.lda")
#Get parameter set of the learner
getParamSet(lrnr)
ps <- makeParamSet(makeNumericParam( "nu",     lower = 2, upper = 10),
                   makeNumericParam( "tol",    lower = 0, upper = 0.0005),
                   makeDiscreteParam("predict.method",values = c("plug-in","predictive","debiased"))
)
lrnr = makeTuneWrapper(lrnr, resampling = inner, par.set = ps, control = ctrl)
modw = train(lrnr, clsf.task)
print(getTuneResult(modw))
#r = resample(lrnr, clsf.task, outer, extract = getTuneResult)
#print(r$extract)

#using the yuned parameters to make the learner
lrnr<-makeLearner("classif.lda",
                  par.vals = list(nu=10,
                                  tot=0,
                                  predict.method="predictive"))

#Defining learner model
lrn_w4c3<- makeFeatSelWrapper(learner    = lrnr, 
                              resampling = cv.folds,
                              control    = control1, 
                              show.info  = FALSE)


#Training of model to extract features
mod_w4c3 = mlr::train(lrn_w4c3, task = clsf.task)

#Extract the features
sfeats = getFeatSelResult(mod_w4c3)

cols<-sfeats$x
cols<-c(cols,"response")
#print(cols)

#dataframe with selected features
df_tune_w4c3<-df_samp_cls
df_tune_w4c3<-df_tune_w4c3[, names(df_tune_w4c3) %in% cols] 
ncol(df_tune_w4c3)

#Getting Accuracy ~ Model training & Testing with subset of features
res_w4c3<-modeltest(df_tune_w4c3, 'C')
print(res_w4c3)

#Capturing best values of hyperparameter after tuning
df_val_w4c3<-data.frame(Observation=character(), Value=character())

nwln<- list(Observation= "No. of Features", Value = (ncol(df_tune_w4c3)-1))
df_val_w4c3 = rbind(df_val_w4c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "nu", Value = tuned.model_c3$x$nu)
df_val_w4c3 = rbind(df_val_w4c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "tol", Value = tuned.model_c3$x$tol)
df_val_w4c3 = rbind(df_val_w4c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "predict.method", Value = tuned.model_c3s$x$predict.method)
df_val_w4c3 = rbind(df_val_w4c3,nwln, stringsAsFactors=FALSE)

nwln<- list(Observation= "Accuracy", Value = res_w4c3)
df_val_w4c3 = rbind(df_val_w4c3,nwln, stringsAsFactors=FALSE)

print(df_val_w4c3)

#---------End of Linear Discriminant Analysis------------
#--------------------------------------------------------
```



